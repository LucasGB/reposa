{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import pymongo\n",
    "\n",
    "#from SentiCR.SentiCR import SentiCR\n",
    "from SentiSW.code.classification.classifier import Classifier\n",
    "from SentiSW.code.entity.training_set_generation import get_entity\n",
    "from git_crawler import GitCrawler\n",
    "\n",
    "users = []\n",
    "tokens = []\n",
    "user = ''\n",
    "token = ''\n",
    "\n",
    "'''\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "database = mongo_client[\"reposadb\"]\n",
    "pull_requests_collection = database[\"pull_requests\"]\n",
    "repositories_collection = database[\"repositories\"]\n",
    "'''\n",
    "\n",
    "with open('auth.txt', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        user, token = line.split(':')\n",
    "        users.append(user.replace('\\n', ''))\n",
    "        tokens.append(token.replace('\\n', ''))\n",
    "\n",
    "repositories = []\n",
    "\n",
    "def init():\n",
    "    print('Initialising')\n",
    "\n",
    "    with open('repositories.txt', 'r') as file:\n",
    "        repositories = file.read().splitlines()\n",
    "\n",
    "    for repository in repositories:\n",
    "        owner, name = repository.split(\"/\")\n",
    "        query = { \"_id\" : \"{}/{}\".format(owner, name) }\n",
    "        documento = list(repositories_collection.find(query))\n",
    "\n",
    "        if(len(documento) == 0):\n",
    "            repositories_collection.insert_one({\n",
    "                                                \"_id\" : \"{}/{}\".format(owner, name),\n",
    "                                                \"owner\": owner,\n",
    "                                                \"name\" : name,\n",
    "                                                'open_pull_requests' : [],\n",
    "                                                'closed_pull_requests' : []\n",
    "                                            })\n",
    "\n",
    "def get_tuples(texts):\n",
    "    #sentiment_analyzer = Classifier(read=False, vector_method='tfidf')\n",
    "    #sentiment_analyzer.save_model()\n",
    "    sentiment_analyzer = Classifier(read=True, vector_method='tfidf')\n",
    "    sentiments = sentiment_analyzer.get_sentiment_polarity_collection(texts)\n",
    "\n",
    "    tuples = []\n",
    "    i = 0\n",
    "    for sentiment in sentiments:\n",
    "        t = {'sentiment': sentiment[0]}\n",
    "        if sentiment != 'Neutral':\n",
    "            entity = get_entity(texts[i])\n",
    "            t['entity'] = entity\n",
    "        else:\n",
    "            t['entity'] = None\n",
    "        tuples.append(t)\n",
    "        i = i + 1\n",
    "    return tuples\n",
    "\n",
    "def get_tuple(text):\n",
    "    #sentiment_analyzer = Classifier(read=False, vector_method='tfidf')\n",
    "    #sentiment_analyzer.save_model()\n",
    "    sentiment_analyzer = Classifier(read=True, vector_method='tfidf')\n",
    "    sentiment = sentiment_analyzer.get_sentiment_polarity(text)[0]\n",
    "    ret = {'sentiment': sentiment}\n",
    "    if sentiment != 'Neutral':\n",
    "        entity = get_entity(text)\n",
    "        ret['entity'] = entity\n",
    "    else:\n",
    "        ret['entity'] = None\n",
    "\n",
    "    return ret\n",
    "\n",
    "def classify(sentences):\n",
    "\n",
    "    saved_SentiCR_model = 'classifier_models/SentiCR_model.sav'\n",
    "    \n",
    "    if(os.path.exists(saved_SentiCR_model)):\n",
    "      sentiment_analyzer = pickle.load(open(saved_SentiCR_model, 'rb'))\n",
    "      print('Loaded SentiCR model')\n",
    "    else:\n",
    "      sentiment_analyzer = SentiCR.SentiCR()\n",
    "      pickle.dump(sentiment_analyzer, open(saved_SentiCR_model, 'wb'))\n",
    "      print('Saved model to file')\n",
    "\n",
    "    for sent in sentences:\n",
    "        score = sentiment_analyzer.get_sentiment_polarity(sent)\n",
    "        print(sent+\"\\n Score: \"+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "init()\n",
    "\n",
    "name = 'rails/rails'\n",
    "        \n",
    "try:\n",
    "    crawler = GitCrawler(name)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(crawler.update_repository())\n",
    "except: \n",
    "    pass\n",
    "finally:\n",
    "    loop.close()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "formatted_elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "print(formatted_elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "MAX_CLASSIFY = 100\n",
    "\n",
    "init()\n",
    "\n",
    "# Select rows by absence of positive_reviews_count field. Indicating if the row has been classified or not\n",
    "repository_query = {\"repository_id\": args.repo, \"positive_reviews_count\" : {\"$exists\" : False} }\n",
    "            \n",
    "#repository_query = {\"repository_id\": args.repo}\n",
    "            \n",
    "#repository_query = {\"repository_id\": \"tensorflow/tensorflow\", \"number\" : 792 }\n",
    "            \n",
    "prs = list(pull_requests_collection.find(repository_query).limit(int(MAX_CLASSIFY)))\n",
    "            \n",
    "for pr in prs:\n",
    "    review_comments_ids = []\n",
    "    review_comments = []\n",
    "    issue_comments_ids = []\n",
    "    issue_comments = []\n",
    "\n",
    "    print('Classifying comments from Pull Request #{}'.format(pr['number']))\n",
    "    k = 1\n",
    "    for review_comment in pr['review_comments']:\n",
    "        review_comments_ids.append(review_comment['_id'])\n",
    "        review_comments.append(review_comment['body'])\n",
    "        print(\"{} - {}\".format(k, review_comment['body']))\n",
    "        k += 1\n",
    "\n",
    "\n",
    "    l = 1\n",
    "    for issue_comment in pr['issue_comments']:\n",
    "        issue_comments_ids.append(issue_comment['_id'])\n",
    "        issue_comments.append(issue_comment['body'])\n",
    "        print(\"{} - {}\".format(l, issue_comment['body']))\n",
    "        l += 1\n",
    "\n",
    "    i = 0\n",
    "    review_positive = 0\n",
    "    review_neutral = 0\n",
    "    review_negative = 0\n",
    "    review_sentiments = get_tuples(review_comments)\n",
    "    for sentiment in review_sentiments:\n",
    "        if(sentiment['sentiment'] == 'Positive'):\n",
    "            review_positive += 1\n",
    "        elif(sentiment['sentiment'] == 'Neutral'):\n",
    "            review_neutral += 1\n",
    "        elif(sentiment['sentiment'] == 'Negative'):\n",
    "            review_negative += 1\n",
    "\n",
    "        pull_requests_collection.update_one({\"repository_id\" : args.repo, 'number' : pr['number'], 'review_comments._id' : review_comments_ids[i] }, { \"$set\" : {\"review_comments.$.sentiment\" : sentiment['sentiment'], \"review_comments.$.entity\" : sentiment['entity']} })\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    issue_positive = 0\n",
    "    issue_neutral = 0\n",
    "    issue_negative = 0\n",
    "    issue_sentiments = get_tuples(issue_comments)\n",
    "\n",
    "    for sentiment in issue_sentiments:\n",
    "        if(sentiment['sentiment'] == 'Positive'):\n",
    "            issue_positive += 1\n",
    "        elif(sentiment['sentiment'] == 'Neutral'):\n",
    "            issue_neutral += 1\n",
    "        elif(sentiment['sentiment'] == 'Negative'):\n",
    "            issue_negative += 1\n",
    "        pull_requests_collection.update_one({\"repository_id\" : args.repo, 'number' : pr['number'], 'issue_comments._id' : issue_comments_ids[i] }, { \"$set\" : {\"issue_comments.$.sentiment\" : sentiment['sentiment'], \"issue_comments.$.entity\" : sentiment['entity']} })\n",
    "        i += 1\n",
    "    \n",
    "    pull_requests_collection.update_one({\"repository_id\" : args.repo, 'number' : pr['number']}, { '$set' : {'positive_reviews_count' : review_positive, 'neutral_reviews_count' : review_neutral, 'negative_reviews_count' : review_negative, 'positive_comments_count' : issue_positive, 'neutral_comments_count' : issue_neutral, 'negative_comments_count' : issue_negative, 'total_positive_count' : review_positive + issue_positive, 'total_neutral_count' : review_neutral + issue_neutral, 'total_negative_count' : review_negative + issue_negative} })\n",
    "\n",
    "    if(int(pr[\"comments_count\"]) < (issue_positive + issue_negative + issue_neutral)):\n",
    "        print(\"Algo de errado nao esta certo.\")\n",
    "        break\n",
    "\n",
    "    print(\"Review comments length: {}; Pos: {}; Neu: {}; Neg: {}\\nIssue comments length: {}; Pos: {}; Neu: {}; Neg: {}\\n\".format(len(review_comments), review_positive, review_neutral, review_negative, len(issue_comments), issue_positive, issue_neutral, issue_negative))\n",
    "    \n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "formatted_elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "print(formatted_elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCC",
   "language": "python",
   "name": "tcc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
